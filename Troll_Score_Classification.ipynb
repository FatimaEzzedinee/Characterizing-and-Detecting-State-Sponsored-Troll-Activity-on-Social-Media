{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Kw2horw56V-Z",
        "9EeMSz3p62uy",
        "etnDxFkL7tni"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "9PF4YG4g6MPp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzmFuC3nOFP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4e4803-cba7-4d50-c9bf-85fc6862b83c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "import sklearn\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Computed Scores"
      ],
      "metadata": {
        "id": "AXH-_y7L6REp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OusJqiIOlRl"
      },
      "source": [
        "scores = pd.read_csv('/content/drive/MyDrive/CV5/scores_train.csv')  #get the computed trolls scores of training set\n",
        "scores_test = pd.read_csv('/content/drive/MyDrive/CV5/scores_test.csv')  #get the computed trolls scores of test set\n",
        "scores = scores.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "Kw2horw56V-Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESUupkCyP1u-"
      },
      "source": [
        "def label_by_score(df, threshold):   #function that get the data and classify the accounts based on the threshold\n",
        "  check_predictions=[]\n",
        "  \n",
        "  for i in range(len(df)):\n",
        "    if df.loc[i]['troll_score'] <= threshold:\n",
        "      check_predictions.append(0)\n",
        "    else:\n",
        "      check_predictions.append(1)\n",
        "\n",
        "  check_lab=df['label'].values\n",
        "\n",
        "  acc=sklearn.metrics.accuracy_score(check_lab, check_predictions)      #accuracy\n",
        "  prec = sklearn.metrics.precision_score(check_lab, check_predictions)  #precision\n",
        "  rec = sklearn.metrics.recall_score(check_lab, check_predictions)      #recall\n",
        "  f1 = sklearn.metrics.f1_score(check_lab, check_predictions)           #f1-score\n",
        "\n",
        "  tn, fp, fn, tp = confusion_matrix(check_lab,check_predictions).ravel()\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(check_lab,check_predictions)\n",
        "  auc = metrics.auc(fpr, tpr)  #auc \n",
        "\n",
        "  tpr = tp/float(tp+fn)\n",
        "  tnr = tn/float(tn+fp)\n",
        "\n",
        "  return (acc,prec,rec,f1,tpr, tnr, auc)\n",
        "\n",
        "#function that returns the max accuracy, precsion, recall, tpr, tnr, f1, auc with their corresponding threshold\n",
        "#threshold range between 0 and 1 with a step of 0.02\n",
        "def get_all_values(scores):   \n",
        "  threshold=0\n",
        "\n",
        "  max_acc_thr=0\n",
        "  max_prec_thr=0\n",
        "  max_rec_thr=0\n",
        "  max_tpr_thr=0\n",
        "  max_tnr_thr=0\n",
        "  max_f1_thr=0\n",
        "  max_auc_thr=0\n",
        "\n",
        "\n",
        "  max_acc=0\n",
        "  max_prec=0\n",
        "  max_rec=0\n",
        "  max_tpr=0\n",
        "  max_tnr=0\n",
        "  max_f1=0\n",
        "  max_auc=0\n",
        "\n",
        "  while threshold <=1:\n",
        "    acc,prec,rec,f1,tpr, tnr, auc = label_by_score(scores, threshold)\n",
        "\n",
        "    if acc >= max_acc:\n",
        "      max_acc=acc\n",
        "      max_acc_thr = threshold\n",
        "\n",
        "    if prec >= max_prec:\n",
        "      max_prec=prec\n",
        "      max_prec_thr = threshold\n",
        "\n",
        "    if rec >= max_rec:\n",
        "      max_rec=rec\n",
        "      max_rec_thr = threshold\n",
        "    \n",
        "    if tpr >= max_tpr:\n",
        "      max_tpr=tpr\n",
        "      max_tpr_thr = threshold\n",
        "    \n",
        "    if tnr >= max_tnr:\n",
        "      max_tnr=tnr\n",
        "      max_tnr_thr = threshold\n",
        "\n",
        "    if f1 >= max_f1:\n",
        "      max_f1=f1\n",
        "      max_f1_thr = threshold\n",
        "    \n",
        "    if auc >= max_auc:\n",
        "      max_auc=auc\n",
        "      max_auc_thr = threshold\n",
        "    \n",
        "    threshold = threshold + 0.02\n",
        "  \n",
        "  return (max_acc_thr, max_prec_thr, max_rec_thr, max_tpr_thr, max_tnr_thr, max_f1_thr, max_auc_thr)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Threshold Computation"
      ],
      "metadata": {
        "id": "9EeMSz3p62uy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1frBe6IEoPg"
      },
      "source": [
        "X = scores['troll_score'].values\n",
        "#X = scores['score'].values\n",
        "y = scores['label'].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXLFKYjYEhsk"
      },
      "source": [
        "#calculating the best acc, prec, .. and saving the performance with stratified k-fold\n",
        "#metrics\n",
        "Acc_a =[]\n",
        "Pre_a =[]\n",
        "Rec_a = []\n",
        "Auc_a = []\n",
        "F1_a =[]\n",
        "TPR_a = []\n",
        "TNR_a = []\n",
        "\n",
        "\n",
        "Acc_auc =[]\n",
        "Pre_auc =[]\n",
        "Rec_auc = []\n",
        "Auc_auc = []\n",
        "F1_auc =[]\n",
        "TPR_auc = []\n",
        "TNR_auc = []\n",
        "\n",
        "\n",
        "Acc_prec=[]\n",
        "Pre_prec =[]\n",
        "Rec_prec = []\n",
        "Auc_prec = []\n",
        "F1_prec =[]\n",
        "TPR_prec = []\n",
        "TNR_prec = []\n",
        "\n",
        "Acc_rec=[]\n",
        "Pre_rec =[]\n",
        "Rec_rec = []\n",
        "Auc_rec = []\n",
        "F1_rec =[]\n",
        "TPR_rec = []\n",
        "TNR_rec = []\n",
        "\n",
        "Acc_f1=[]\n",
        "Pre_f1 =[]\n",
        "Rec_f1 = []\n",
        "Auc_f1 = []\n",
        "F1_f1 =[]\n",
        "TPR_f1 = []\n",
        "TNR_f1 = []\n",
        "\n",
        "\n",
        "Acc_tpr=[]\n",
        "Pre_tpr=[]\n",
        "Rec_tpr = []\n",
        "Auc_tpr = []\n",
        "F1_tpr =[]\n",
        "TPR_tpr = []\n",
        "TNR_tpr = []\n",
        "\n",
        "\n",
        "Acc_tnr=[]\n",
        "Pre_tnr=[]\n",
        "Rec_tnr = []\n",
        "Auc_tnr = []\n",
        "F1_tnr =[]\n",
        "TPR_tnr = []\n",
        "TNR_tnr = []\n",
        "\n",
        "\n",
        "Recalls=[]\n",
        "Precisions=[]\n",
        "AUCs=[]\n",
        "Accs=[]\n",
        "TNRs=[]\n",
        "F1s=[]\n",
        "\n",
        "# Instantiate the cross validator\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "# Loop through the indices the split() method returns\n",
        "for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
        "      # Generate batches from indices\n",
        "      x_train, x_test = X[train_indices], X[val_indices]\n",
        "      y_train, y_test = y[train_indices], y[val_indices]\n",
        "\n",
        "      df_train = pd.DataFrame(columns=['troll_score','label'])\n",
        "      df_train['troll_score']=x_train\n",
        "      df_train['label']=y_train\n",
        "\n",
        "      max_acc_thr, max_prec_thr, max_rec_thr, max_tpr_thr, max_tnr_thr, max_f1_thr, max_auc_thr = get_all_values(df_train) #get the best performance threshold for the train set\n",
        "\n",
        "      Recalls.append(max_rec_thr)\n",
        "      Precisions.append(max_prec_thr)\n",
        "      AUCs.append(max_auc_thr)\n",
        "      Accs.append(max_acc_thr)\n",
        "      TNRs.append(max_tnr_thr)\n",
        "      F1s.append(max_f1_thr)\n",
        "\n",
        "      df_test=pd.DataFrame(columns=['troll_score','label'])\n",
        "      df_test['troll_score']=x_test\n",
        "      df_test['label']=y_test\n",
        "\n",
        "      # performance with threshold of best accuracy\n",
        "      acc,prec,rec,f1,tpr, tnr, auc = label_by_score(df_test,max_acc_thr)  \n",
        "      #save the performance\n",
        "      Acc_a.append(acc)\n",
        "      Pre_a.append(prec)\n",
        "      Rec_a.append(rec)\n",
        "      Auc_a.append(auc)\n",
        "      F1_a.append(f1)\n",
        "      TPR_a.append(tpr)\n",
        "      TNR_a.append(tnr)\n",
        "      \n",
        "    # performance with threshold of best precision\n",
        "      acc,prec,rec,f1,tpr, tnr, auc = label_by_score(df_test,max_prec_thr)\n",
        "\n",
        "      Acc_prec.append(acc)\n",
        "      Pre_prec.append(prec)\n",
        "      Rec_prec.append(rec)\n",
        "      Auc_prec.append(auc)\n",
        "      F1_prec.append(f1)\n",
        "      TPR_prec.append(tpr)\n",
        "      TNR_prec.append(tnr)\n",
        "\n",
        "      # performance with threshold of best recall\n",
        "      acc,prec,rec,f1,tpr, tnr, auc = label_by_score(df_test,max_rec_thr)\n",
        "\n",
        "      Acc_rec.append(acc)\n",
        "      Pre_rec.append(prec)\n",
        "      Rec_rec.append(rec)\n",
        "      Auc_rec.append(auc)\n",
        "      F1_rec.append(f1)\n",
        "      TPR_rec.append(tpr)\n",
        "      TNR_rec.append(tnr)\n",
        "\n",
        "      # performance with threshold of best tpr (recall)\n",
        "      acc,prec,rec,f1,tpr, tnr, auc = label_by_score(df_test,max_tpr_thr)\n",
        "\n",
        "      Acc_tpr.append(acc)\n",
        "      Pre_tpr.append(prec)\n",
        "      Rec_tpr.append(rec)\n",
        "      Auc_tpr.append(auc)\n",
        "      F1_tpr.append(f1)\n",
        "      TPR_tpr.append(tpr)\n",
        "      TNR_tpr.append(tnr)\n",
        "\n",
        "      # performance with threshold of best TNR (true negative rate)\n",
        "\n",
        "      acc,prec,rec,f1,tpr, tnr, auc = label_by_score(df_test,max_tnr_thr)\n",
        "\n",
        "      Acc_tnr.append(acc)\n",
        "      Pre_tnr.append(prec)\n",
        "      Rec_tnr.append(rec)\n",
        "      Auc_tnr.append(auc)\n",
        "      F1_tnr.append(f1)\n",
        "      TPR_tnr.append(tpr)\n",
        "      TNR_tnr.append(tnr)\n",
        "      # performance with threshold of best f1-score\n",
        "      acc,prec,rec,f1,tpr, tnr, auc = label_by_score(df_test,max_f1_thr)\n",
        "\n",
        "      Acc_f1.append(acc)\n",
        "      Pre_f1.append(prec)\n",
        "      Rec_f1.append(rec)\n",
        "      Auc_f1.append(auc)\n",
        "      F1_f1.append(f1)\n",
        "      TPR_f1.append(tpr)\n",
        "      TNR_f1.append(tnr)\n",
        "      # performance with threshold of best AUC (Area under the curve)\n",
        "      acc,prec,rec,f1,tpr, tnr, auc = label_by_score(df_test,max_auc_thr)\n",
        "\n",
        "      Acc_auc.append(acc)\n",
        "      Pre_auc.append(prec)\n",
        "      Rec_auc.append(rec)\n",
        "      Auc_auc.append(auc)\n",
        "      F1_auc.append(f1)\n",
        "      TPR_auc.append(tpr)\n",
        "      TNR_auc.append(tnr)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display Averaged Results"
      ],
      "metadata": {
        "id": "etnDxFkL7tni"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iLuS6MqJAQA"
      },
      "source": [
        "print(\"Best Acc\")\n",
        "print(\"accuracy\",np.mean(Acc_a),np.std(Acc_a))\n",
        "print(\"precision\",np.mean(Pre_a),np.std(Pre_a))\n",
        "print(\"recall\",np.mean(Rec_a),np.std(Rec_a))\n",
        "print(\"f1\",np.mean(F1_a),np.std(F1_a))\n",
        "print(\"AUC\",np.mean(Auc_a),np.std(Auc_a))\n",
        "print(\"TPR\",np.mean(TPR_a),np.std(TPR_a))\n",
        "print(\"TNR\",np.mean(TNR_a),np.std(TNR_a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-Org0oIpep"
      },
      "source": [
        "print(\"Best AUC\")\n",
        "print(\"accuracy\",np.mean(Acc_auc),np.std(Acc_auc))\n",
        "print(\"precision\",np.mean(Pre_auc),np.std(Pre_auc))\n",
        "print(\"recall\",np.mean(Rec_auc),np.std(Rec_auc))\n",
        "print(\"f1\",np.mean(F1_auc),np.std(F1_auc))\n",
        "print(\"AUC\",np.mean(Auc_auc),np.std(Auc_auc))\n",
        "print(\"TPR\",np.mean(TPR_auc),np.std(TPR_auc))\n",
        "print(\"TNR\",np.mean(TNR_auc),np.std(TNR_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqvmVq-KJWvY"
      },
      "source": [
        "print(\"Best Recall\")\n",
        "print(\"accuracy\",np.mean(Acc_rec),np.std(Acc_rec))\n",
        "print(\"precision\",np.mean(Pre_rec),np.std(Pre_rec))\n",
        "print(\"recall\",np.mean(Rec_rec),np.std(Rec_rec))\n",
        "print(\"f1\",np.mean(F1_rec),np.std(F1_rec))\n",
        "print(\"AUC\",np.mean(Auc_rec),np.std(Auc_rec))\n",
        "print(\"TPR\",np.mean(TPR_rec),np.std(TPR_rec))\n",
        "print(\"TNR\",np.mean(TNR_rec),np.std(TNR_rec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqlJbDVWJqef"
      },
      "source": [
        "print(\"Best Precision\")\n",
        "print(\"accuracy\",np.mean(Acc_prec),np.std(Acc_prec))\n",
        "print(\"precision\",np.mean(Pre_prec),np.std(Pre_prec))\n",
        "print(\"recall\",np.mean(Rec_prec),np.std(Rec_prec))\n",
        "print(\"f1\",np.mean(F1_prec),np.std(F1_prec))\n",
        "print(\"AUC\",np.mean(Auc_prec),np.std(Auc_prec))\n",
        "print(\"TPR\",np.mean(TPR_prec),np.std(TPR_prec))\n",
        "print(\"TNR\",np.mean(TNR_prec),np.std(TNR_prec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nggw4cQvJ3UL"
      },
      "source": [
        "print(\"Best F1-score\")\n",
        "print(\"accuracy\",np.mean(Acc_f1),np.std(Acc_f1))\n",
        "print(\"precision\",np.mean(Pre_f1),np.std(Pre_f1))\n",
        "print(\"recall\",np.mean(Rec_f1),np.std(Rec_f1))\n",
        "print(\"f1\",np.mean(F1_f1),np.std(F1_f1))\n",
        "print(\"AUC\",np.mean(Auc_f1),np.std(Auc_f1))\n",
        "print(\"TPR\",np.mean(TPR_f1),np.std(TPR_f1))\n",
        "print(\"TNR\",np.mean(TNR_f1),np.std(TNR_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9sP7cxeJ_8v"
      },
      "source": [
        "print(\"Best TNR\")\n",
        "print(\"accuracy\",np.mean(Acc_tnr),np.std(Acc_tnr))\n",
        "print(\"precision\",np.mean(Pre_tnr),np.std(Pre_tnr))\n",
        "print(\"recall\",np.mean(Rec_tnr),np.std(Rec_tnr))\n",
        "print(\"f1\",np.mean(F1_tnr),np.std(F1_tnr))\n",
        "print(\"AUC\",np.mean(Auc_tnr),np.std(Auc_tnr))\n",
        "print(\"TPR\",np.mean(TPR_tnr),np.std(TPR_tnr))\n",
        "print(\"TNR\",np.mean(TNR_tnr),np.std(TNR_tnr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUCs, np.average(AUCs)"
      ],
      "metadata": {
        "id": "lfEn2Y3LINlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc,prec,rec,f1,tpr, tnr, auc = label_by_score(scores_test,np.average(AUCs))  "
      ],
      "metadata": {
        "id": "oO-DpDrdA29g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance on Test Set\")\n",
        "print(\"accuracy\",acc)\n",
        "print(\"precision\",prec)\n",
        "print(\"recall\",rec)\n",
        "print(\"f1\",f1)\n",
        "print(\"AUC\",auc)\n",
        "print(\"TPR\",tpr)\n",
        "print(\"TNR\",tnr)"
      ],
      "metadata": {
        "id": "J0Lxz66BH_KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-PAvofRVIgqc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}