{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy9xLUgwZiMa"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVLIbWft_e2K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.callbacks import EarlyStopping\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Eix89JZnzM"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(NT,nt)   A - 0\n",
        "#(NT,rp)   B - 1\n",
        "#(NT,rt)   C - 2\n",
        "#(NT,tw)   D - 3\n",
        "\n",
        "#(RP,nt)   E - 4\n",
        "#(RP,rp)   F - 5\n",
        "#(RP,rt)   G - 6\n",
        "#(RP,tw)   H - 7\n",
        "   \n",
        "#(RT,nt)   I - 8\n",
        "#(RT,rp)   J - 9\n",
        "#(RT,rt)   K - 10\n",
        "#(RT,tw)   L - 11\n",
        "\n",
        "def reconstruct_traj(row):\n",
        "    f_i=row\n",
        "    f_i=f_i[1:]\n",
        "    f_i = f_i[:-1]\n",
        "    s=str(f_i)\n",
        "    s=s.replace(\" \", \"\")\n",
        "    x = s.split(\"[\")\n",
        "\n",
        "    t=[]\n",
        "    for i in range(0,len(x)):\n",
        "        r=x[i].replace(\"]\", \"\").split(\",\")\n",
        "        if len(r) >=2:\n",
        "            t.append([r[0],r[1]])\n",
        "    \n",
        "    return t\n",
        "\n",
        "def remove_action(traj):   ##keep only the S_A pair\n",
        "    seq=[]\n",
        "    #print(traj)\n",
        "    for i in range(len(traj)):\n",
        "        seq.append(int(traj[i][0]))\n",
        "    return seq\n",
        "\n",
        "users_traj = pd.read_csv(\"/content/drive/MyDrive/Trajectories+code/users_all_trajectories.csv\") \n",
        "trolls_traj=pd.read_csv(\"/content/drive/MyDrive/Trajectories+code/trolls_all_trajectories.csv\")\n",
        "\n",
        "users_traj['state_sequence'] = users_traj.apply(lambda row : reconstruct_traj(row['state_sequence']), axis = 1)\n",
        "trolls_traj['state_sequence'] = trolls_traj.apply(lambda row : reconstruct_traj(row['state_sequence']), axis = 1)\n",
        "\n",
        "users_traj['sequence_numbers'] = users_traj.apply(lambda row : remove_action(row['state_sequence']), axis = 1)\n",
        "trolls_traj['sequence_numbers'] = trolls_traj.apply(lambda row : remove_action(row['state_sequence']), axis = 1)\n",
        "\n",
        "users_traj['label'] = 0\n",
        "trolls_traj['label']= 1\n",
        "\n",
        "frames = [users_traj, trolls_traj]\n",
        "data = pd.concat(frames)\n",
        "data = data.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "mX2Qm2r7TCpe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQC28RbqPebz",
        "outputId": "1cf1cd88-5bd8-4e4e-a64e-54af1316a51a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2322"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train - Test Split over all users/trolls\n",
        "data_train, data_test = train_test_split(data, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "74S3DldcPbps"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qGxXwm2ZL_Z"
      },
      "source": [
        "### Trajectory Formation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TdXPiCdEe_Pd"
      },
      "outputs": [],
      "source": [
        "#preparing the splitted sequences of trainig users/trolls\n",
        "users = data_train[data_train['label']==0].reset_index(drop=True)\n",
        "trolls = data_train[data_train['label']==1].reset_index(drop=True)\n",
        "max_length = 200 #trajectory Length: L. LSTM input\n",
        "new_data = pd.DataFrame(columns=['sequence_numbers','label'])\n",
        "count=0\n",
        "#users\n",
        "#iterating over users from training set\n",
        "for i in range(len(users)):\n",
        "  user = users.loc[i]\n",
        "  traj = users.loc[i]['sequence_numbers']\n",
        "  l = len(traj)\n",
        "\n",
        "  if l <= max_length: #if trajectory length less then max_lenght add it as it is\n",
        "    res=traj\n",
        "    new_data.loc[count]=[res,0] # label 0 for users\n",
        "    count=count+1\n",
        "\n",
        "  else:  #else split it into records\n",
        "    res = np.array_split(traj[0:(len(traj)-len(traj)%max_length)],len(traj)/max_length)\n",
        "    for j in range(len(res)):\n",
        "      new_data.loc[count]=[res[j].tolist(),0] # label 0 for users\n",
        "      count=count+1\n",
        "      \n",
        "new_data_trolls=pd.DataFrame(columns=['sequence_numbers','label'])\n",
        "\n",
        "count=0\n",
        "#trolls\n",
        "#iterating over trolls from training set\n",
        "for i in range(len(trolls)):\n",
        "  troll=trolls.loc[i]\n",
        "  traj = trolls.loc[i]['sequence_numbers']\n",
        "  l = len(traj)\n",
        "\n",
        "  if l <= max_length:  #if trajectory length less then max_lenght add it as it is\n",
        "    res=traj\n",
        "    new_data_trolls.loc[count]=[res,1]\n",
        "    count=count+1\n",
        "\n",
        "  else: #else split it into records\n",
        "    res = np.array_split(traj[0:(len(traj)-len(traj)%max_length)],len(traj)/max_length)\n",
        "    for j in range(len(res)):\n",
        "      new_data_trolls.loc[count]=[res[j].tolist(),1]\n",
        "      count=count+1\n",
        "\n",
        "#all_data is the dataframe that contains the extracted sequences of users and trolls from train set\n",
        "frames = [new_data, new_data_trolls]\n",
        "all_train_data = pd.concat(frames)\n",
        "all_train_data = all_train_data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F05PgNHTxrCZ"
      },
      "outputs": [],
      "source": [
        "print (\" After Splitting Training Set  --> Users : \" + str(len(new_data)) +\" Trolls : \" +str(len(new_data_trolls)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same for test set extracting the trajectories\n",
        "#preparing the splitted sequences\n",
        "users = data_test[data_test['label']==0].reset_index(drop=True)\n",
        "trolls = data_test[data_test['label']==1].reset_index(drop=True)\n",
        "max_length = 200 #trajectory Length: L. LSTM input\n",
        "new_data = pd.DataFrame(columns=['sequence_numbers','label'])\n",
        "count=0\n",
        "#users\n",
        "for i in range(len(users)):\n",
        "  user = users.loc[i]\n",
        "  traj = users.loc[i]['sequence_numbers']\n",
        "  l = len(traj)\n",
        "\n",
        "  if l <= max_length: #if trajectory length less then max_lenght add it as it is\n",
        "    res=traj\n",
        "    new_data.loc[count]=[res,0]\n",
        "    count=count+1\n",
        "\n",
        "  else:  #else split it into records\n",
        "    res = np.array_split(traj[0:(len(traj)-len(traj)%max_length)],len(traj)/max_length)\n",
        "    for j in range(len(res)):\n",
        "      new_data.loc[count]=[res[j].tolist(),0]\n",
        "      count=count+1\n",
        "      \n",
        "new_data_trolls=pd.DataFrame(columns=['sequence_numbers','label'])\n",
        "\n",
        "count=0\n",
        "#trolls\n",
        "#iterating over trolls\n",
        "for i in range(len(trolls)):\n",
        "  troll=trolls.loc[i]\n",
        "  traj = trolls.loc[i]['sequence_numbers']\n",
        "  l = len(traj)\n",
        "\n",
        "  if l <= max_length:  #if trajectory length less then max_lenght add it as it is\n",
        "    res=traj\n",
        "    new_data_trolls.loc[count]=[res,1]\n",
        "    count=count+1\n",
        "\n",
        "  else: #else split it into records\n",
        "    res = np.array_split(traj[0:(len(traj)-len(traj)%max_length)],len(traj)/max_length)\n",
        "    for j in range(len(res)):\n",
        "      new_data_trolls.loc[count]=[res[j].tolist(),1]\n",
        "      count=count+1\n",
        "\n",
        "#all_data is the dataframe that contains the extracted sequences\n",
        "frames = [new_data, new_data_trolls]\n",
        "all_test_data = pd.concat(frames)\n",
        "all_test_data = all_test_data.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "prMVEiAaPueF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\" After Splitting Test Set--> Users : \" + str(len(new_data)) +\" Trolls : \" +str(len(new_data_trolls)))"
      ],
      "metadata": {
        "id": "ieb2-BrXQGy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trajectory Classification\n",
        "Training with 85% train and 15% test"
      ],
      "metadata": {
        "id": "Ap2AOnT5aEIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9F6Z-jv6P3I"
      },
      "outputs": [],
      "source": [
        "Xx=all_train_data['sequence_numbers'].values  #trajectories\n",
        "Y=all_train_data['label'].values\n",
        "\n",
        "max_length = 200\n",
        "X = pad_sequences(Xx, maxlen=max_length, padding='post')  ## fill 0s\n",
        "# Split data into 85% training & 15% test\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_validation.shape, y_validation.shape)\n",
        "\n",
        "x_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "x_validation=x_validation.reshape(x_validation.shape[0],x_validation.shape[1],1)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_validation.shape, y_validation.shape)\n",
        "\n",
        "x_train=np.asarray(x_train).astype(np.int)\n",
        "y_train=np.asarray(y_train).astype(np.int)\n",
        "\n",
        "x_validation=np.asarray(x_validation).astype(np.int)\n",
        "y_validation=np.asarray(y_validation).astype(np.int)\n",
        "\n",
        "\n",
        "# build the neural network\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, return_sequences = True, input_shape = (x_train.shape[1],1 )))\n",
        "model.add(Dropout(rate = 0.2))\n",
        "model.add(LSTM(75,return_sequences = True))\n",
        "model.add(Dropout(rate = 0.2))\n",
        "model.add(LSTM(75,return_sequences = True))\n",
        "model.add(Dropout(rate = 0.2))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dropout(rate = 0.2))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "es = EarlyStopping(monitor='auc', mode='max', patience=10, verbose=1) ## patience can be varied\n",
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics=[\"acc\",\"AUC\",\"FalseNegatives\",\"FalsePositives\",\"TrueNegatives\",\"TruePositives\",\"Precision\",\"Recall\"])\n",
        "history = model.fit(x = x_train, y = y_train,callbacks=[es], validation_data=(x_validation, y_validation), batch_size = 100, epochs = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU2x-fw-wNrd"
      },
      "outputs": [],
      "source": [
        "x_test = all_test_data['sequence_numbers'].values  #trajectories\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post')  ## fill 0s\n",
        "y_test = all_test_data['label'].values\n",
        "\n",
        "x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "y_test=np.asarray(y_test).astype(np.int)\n",
        "\n",
        "results = model.evaluate(x_test, y_test, batch_size=100)  #model evaluation on the test set\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified Cross validation"
      ],
      "metadata": {
        "id": "cng4dR0zzsrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWcajVP-Zp1W"
      },
      "outputs": [],
      "source": [
        "Xx=all_train_data['sequence_numbers'].values\n",
        "y=all_train_data['label'].values\n",
        "\n",
        "y=np.asarray(y).astype(np.int)\n",
        "max_length = 200\n",
        "X = pad_sequences(Xx, maxlen=max_length, padding='post')\n",
        "\n",
        "#metrics\n",
        "Acc =[]\n",
        "Pre =[]\n",
        "Rec = []\n",
        "Auc = []\n",
        "F1 =[]\n",
        "FI =[]\n",
        "TPR = []\n",
        "TNR = []\n",
        "Loss=[]\n",
        "\n",
        "# Instantiate the cross validator\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "\n",
        "# Loop through the indices the split() method returns\n",
        "for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
        "\n",
        "    # Generate batches from indices\n",
        "    x_train, x_test = X[train_indices], X[val_indices]\n",
        "    y_train, y_test = y[train_indices], y[val_indices]\n",
        "\n",
        "    x_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
        "    x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "\n",
        "    x_train=np.asarray(x_train).astype(np.int)\n",
        "    y_train=np.asarray(y_train).astype(np.int)\n",
        "\n",
        "    x_test=np.asarray(x_test).astype(np.int)\n",
        "    y_test=np.asarray(y_test).astype(np.int)\n",
        "    # build the neural network\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100, return_sequences = True, input_shape = (x_train.shape[1],1 )))\n",
        "    model.add(Dropout(rate = 0.2))\n",
        "    model.add(LSTM(75,return_sequences = True))\n",
        "    model.add(Dropout(rate = 0.2))\n",
        "    model.add(LSTM(75,return_sequences = True))\n",
        "    model.add(Dropout(rate = 0.2))\n",
        "    model.add(LSTM(50))\n",
        "    model.add(Dropout(rate = 0.2))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    #es = EarlyStopping(monitor='auc', mode='max', patience=20, verbose=1)\n",
        "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics=[\"acc\",\"AUC\",\"FalseNegatives\",\"FalsePositives\",\"TrueNegatives\",\"TruePositives\",\"Precision\",\"Recall\"])\n",
        "    model.fit(x = x_train, y = y_train, batch_size = 100,verbose=1, epochs = 500)\n",
        "    history = model.evaluate(x_test, y_test, batch_size=100)\n",
        "    Loss.append(history[0])\n",
        "\n",
        "    acc= history[1]\n",
        "    Acc.append(acc)\n",
        "\n",
        "    auc = history[2]\n",
        "    Auc.append(auc)\n",
        "\n",
        "    fn= history[3]\n",
        "    fp = history[4]\n",
        "    tn = history[5]\n",
        "    tp = history[6]\n",
        "\n",
        "\n",
        "    tpr = tp/float(tp+fn)\n",
        "    tnr = tn/float(tn+fp)\n",
        "\n",
        "    TPR.append(tpr)\n",
        "    TNR.append(tnr)\n",
        "\n",
        "    prec= history[7]\n",
        "    Pre.append(prec)\n",
        "\n",
        "    rec = history[8]\n",
        "    Rec.append(rec)\n",
        "    try:\n",
        "      f1 = 2 * ((prec*rec)/(prec+rec))\n",
        "    except ZeroDivisionError:\n",
        "      f1 = 0\n",
        "    F1.append(f1)\n",
        "\n",
        "print(\"LSTM With Trajectory classification\")\n",
        "print(\"accuracy\",np.mean(Acc),np.std(Acc))\n",
        "print(\"precision\",np.mean(Pre),np.std(Pre))\n",
        "print(\"recall\",np.mean(Rec),np.std(Rec))\n",
        "print(\"f1\",np.mean(F1),np.std(F1))\n",
        "print(\"AUC\",np.mean(Auc),np.std(Auc))\n",
        "print(\"TPR\",np.mean(TPR),np.std(TPR))\n",
        "print(\"TNR\",np.mean(TNR),np.std(TNR))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Troll Score Computation and CDF plot"
      ],
      "metadata": {
        "id": "IHdh08_Y0koi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### sliding window validation\n",
        "\n",
        "### troll score classification:  e.g: input [0, 0, 0, 0, 0], mask [1, 1, 1, 1, 1] => troll score is the number of matched 1's\n",
        "\n",
        "max_length=200\n",
        "\n",
        "output=pd.DataFrame(columns=['screen_name','sequence_numbers','label','troll_score'])\n",
        "\n",
        "counter=0\n",
        "users_acc=[]\n",
        "trolls_acc=[]\n",
        "\n",
        "users = data_test[data_test['label']==0].reset_index(drop=True)\n",
        "trolls = data_test[data_test['label']==1].reset_index(drop=True)\n",
        "\n",
        "#computing the troll score for the users\n",
        "#sliding window\n",
        "#compute the ratio of how many time the sequences classified as troll\n",
        "\n",
        "for i in range(len(users)):\n",
        "  user=users.loc[i]\n",
        "  traj = users.loc[i]['sequence_numbers']\n",
        "  l = len(traj)\n",
        "\n",
        "  if l <= max_length:  # data used in the training will be ignored\n",
        "    res=traj   #then do nothing\n",
        "\n",
        "  else:\n",
        "    res=[]\n",
        "    for j in range(l-max_length -1):\n",
        "      #if j%max_length ==0:     #data used in the training will be ignored\n",
        "      #  continue\n",
        "      res.append(traj[j:j+max_length])\n",
        "\n",
        "    if len(res) > 0:\n",
        "      x_test = pad_sequences(res, maxlen=max_length, padding='post')\n",
        "      x_test= x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "      y_test=np.asarray([1]*len(x_test)).astype(np.int)  \n",
        "      history = model.evaluate(x_test, y_test, batch_size=100)\n",
        "      output.loc[counter]=[user['screen_name'],user['sequence_numbers'],0,history[1]]\n",
        "      counter = counter+1\n",
        "      users_acc.append(history[1])\n",
        "\n",
        "#computing the troll score for the trolls\n",
        "#sliding window\n",
        "#compute the ratio of how many time the sequences classified as troll\n",
        "counter =0\n",
        "for i in range(len(trolls)):\n",
        "  troll=trolls.loc[i]\n",
        "  traj = trolls.loc[i]['sequence_numbers']\n",
        "  l = len(traj)\n",
        "\n",
        "  if l <= max_length:  \n",
        "    res=traj   #do nothing because data used in the training\n",
        "\n",
        "  else:\n",
        "    res=[]\n",
        "    for j in range(l- max_length -1):\n",
        "      #if j%max_length == 0:     #data used in the training will be ignored\n",
        "      #  continue\n",
        "      res.append(traj[j:j+max_length])\n",
        "    if len(res) > 0:\n",
        "      x_test = pad_sequences(res, maxlen=max_length, padding='post')\n",
        "      x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
        "      y_test=np.asarray([1]*len(x_test)).astype(np.int)\n",
        "      history = model.evaluate(x_test, y_test, batch_size=100)\n",
        "      output.loc[counter]=[user['screen_name'],user['sequence_numbers'],1,history[1]]\n",
        "      counter=counter+1\n",
        "      trolls_acc.append(history[1])"
      ],
      "metadata": {
        "id": "_Lx1OxyB0hxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cumulative distribution function"
      ],
      "metadata": {
        "id": "p-F4Eoh5H0cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "\n",
        "mpl.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (5, 5)\n",
        "\n",
        "# normal distribution\n",
        "# sort the data in ascending order\n",
        "# get the cdf values of y\n",
        "# No of data points used\n",
        "N = len(users_acc)\n",
        "x = np.sort(users_acc)\n",
        "y = np.arange(N) / float(N)\n",
        "plt.plot(x, y,color='blue',label='Users')\n",
        "\n",
        "df=pd.DataFrame(columns=['x','y'])\n",
        "df['x']=x\n",
        "df['y']=y\n",
        "\n",
        "df.to_csv(\"/content/drive/My Drive/D/NEW 30-4-2022/CDF/users_cdf_200.csv\")\n",
        "\n",
        "\n",
        "N = len(trolls_acc)\n",
        "x = np.sort(trolls_acc)\n",
        "y = np.arange(N) / float(N)\n",
        "\n",
        "\n",
        "df=pd.DataFrame(columns=['x','y'])\n",
        "df['x']=x\n",
        "df['y']=y\n",
        "\n",
        "df.to_csv(\"/content/drive/My Drive/D/NEW 30-4-2022/CDF/trolls_cdf_200.csv\")\n",
        "plt.plot(x, y,color='red',label='Trolls')\n",
        "\n",
        "font = {'family' : 'normal',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 12}\n",
        "\n",
        "mpl.rc('font', **font)\n",
        "  \n",
        "# plotting\n",
        "plt.xlabel('Trolls score')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "VlMyzaRs08ST"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cng4dR0zzsrl"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}